{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.backends import cudnn\n",
    "import skimage.io\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(api_key=\"[REDACTED]\", project_name=\"obama_video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "channels = 3\n",
    "seq_length = 20\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, seq_length, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.seq_length = seq_length\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted(glob.glob(os.path.join(root_dir, \"*.png\")))\n",
    "    def __len__(self):\n",
    "        return len(self.filenames) - (seq_length - 1)\n",
    "    def __getitem__(self, idx):\n",
    "        images = [skimage.io.imread(self.filenames[idx+i]) for i in range(seq_length)]\n",
    "        if self.transform:\n",
    "            images = list(map(self.transform, images))\n",
    "        else:\n",
    "            images = list(map(transforms.ToTensor(), images))\n",
    "        return torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoDataset(\"/home/santiago/Downloads/obama/images/\", 20, transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(1080),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "workers = 4\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim):\n",
    "        self.img_size = img_size\n",
    "        self.ds_size = self.img_size // 2**5\n",
    "        self.latent_dim = latent_dim\n",
    "        super(VideoAutoencoder, self).__init__()\n",
    "        \n",
    "        self.enc_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 1, 1),\n",
    "            nn.Conv2d(16, 16, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, 1, 1),\n",
    "            nn.Conv2d(32, 32, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.Conv2d(128, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.Conv2d(256, 256, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm2d(256, 0.8)\n",
    "        )\n",
    "        self.enc_proj = nn.Sequential(\n",
    "            nn.Linear(256*self.ds_size**2, latent_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.BatchNorm1d(latent_dim, 0.8)\n",
    "        )\n",
    "        self.enc_lstm = nn.LSTM(latent_dim, latent_dim)\n",
    "        \n",
    "        self.dec_lstm = nn.LSTM(latent_dim, latent_dim)\n",
    "        self.dec_proj = nn.Sequential(\n",
    "            nn.BatchNorm1d(128, 0.8),\n",
    "            nn.Linear(latent_dim, 256*self.ds_size**2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.ConvTranspose2d(256, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.ConvTranspose2d(128, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.BatchNorm2d(16, 0.8),\n",
    "            nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(16, 3, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        enc_reps = []\n",
    "        for step in x.split(1):\n",
    "            step = step[0]\n",
    "            conv = self.enc_conv(step)\n",
    "            conv = conv.view(conv.shape[0], 256*self.ds_size**2)\n",
    "            proj = self.enc_proj(conv)\n",
    "            enc_reps.append(proj)\n",
    "        out, states = self.enc_lstm(torch.stack(enc_reps))\n",
    "        return states\n",
    "    \n",
    "    def decode(self, z, steps):\n",
    "        step = Variable(torch.zeros(1, z[0].shape[1], z[0].shape[2])).cuda()\n",
    "        decoded = []\n",
    "        for i in range(steps):\n",
    "            step, z = self.dec_lstm(step, z)\n",
    "            proj = self.dec_proj(step[0])\n",
    "            proj = proj.view(proj.shape[0], 256, self.ds_size, self.ds_size)\n",
    "            dec = self.dec_conv(proj)\n",
    "            decoded.append(dec)\n",
    "        return torch.stack(list(reversed(decoded)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        y = self.decode(z, x.shape[0])\n",
    "        loss = (F.l1_loss(x, y) / x.numel()) ** 1.1\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoAutoencoder(img_size, latent_dim).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "global_step = 0\n",
    "checkpoint_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with experiment.train():\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs = Variable(data.permute(1, 0, 2, 3, 4)).cuda()  # (t, b, c, h, w)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward(inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            experiment.log_metric(\"loss\", loss.item(), step=global_step)\n",
    "            print(\"(Epoch {}) (Global Step {}) (Loss {})\".format(epoch, global_step, loss.item()), end='\\r', flush=True)\n",
    "            if i % checkpoint_interval == 0:\n",
    "                torch.save(model.state_dict(), \"../obama_video/checkpoints/model_{}.pth\".format(global_step))\n",
    "                torch.save(optimizer.state_dict(), \"../obama_video/checkpoints/optimizer_{}.pth\".format(global_step))\n",
    "            global_step += 1\n",
    "        print(\"Epoch {} done!\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import skimage.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(np.rollaxis(inputs[0, 0, ...].cpu().numpy(), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.encode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model.decode(encoded, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(np.rollaxis(decoded[19, 0, ...].cpu().detach().numpy(), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
